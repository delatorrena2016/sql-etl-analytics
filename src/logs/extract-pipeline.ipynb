{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9466c413",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T03:18:40.636240Z",
     "iopub.status.busy": "2023-10-16T03:18:40.636240Z",
     "iopub.status.idle": "2023-10-16T03:18:40.648537Z",
     "shell.execute_reply": "2023-10-16T03:18:40.648537Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 0.016998,
     "end_time": "2023-10-16T03:18:40.649387",
     "exception": false,
     "start_time": "2023-10-16T03:18:40.632389",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# declare a list tasks whose products you want to use as inputs\n",
    "upstream = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ca5a90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T03:18:40.653388Z",
     "iopub.status.busy": "2023-10-16T03:18:40.653388Z",
     "iopub.status.idle": "2023-10-16T03:18:40.664693Z",
     "shell.execute_reply": "2023-10-16T03:18:40.663854Z"
    },
    "papermill": {
     "duration": 0.013206,
     "end_time": "2023-10-16T03:18:40.664693",
     "exception": false,
     "start_time": "2023-10-16T03:18:40.651487",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "product = {\"nb\": \"C:\\\\Users\\\\padro\\\\OneDrive\\\\Desktop\\\\ploomber\\\\sql-etl-analytics\\\\sql-etl-analytics\\\\src\\\\logs\\\\extract-pipeline.ipynb\", \"data\": \"C:\\\\Users\\\\padro\\\\OneDrive\\\\Desktop\\\\ploomber\\\\sql-etl-analytics\\\\sql-etl-analytics\\\\src\\\\data\\\\trends.duckdb\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a29dc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T03:18:40.667793Z",
     "iopub.status.busy": "2023-10-16T03:18:40.667793Z",
     "iopub.status.idle": "2023-10-16T03:18:44.243631Z",
     "shell.execute_reply": "2023-10-16T03:18:44.242753Z"
    },
    "papermill": {
     "duration": 3.577941,
     "end_time": "2023-10-16T03:18:44.244735",
     "exception": false,
     "start_time": "2023-10-16T03:18:40.666794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Date: 2020-01-01 00:00:00\n",
      "Last Date: 2021-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kaggle\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "\n",
    "# Check if 'data' folder exists, if not, create it\n",
    "# Set the path relative to the script\n",
    "def extract_data(dataset_id, data_dir):\n",
    "    \"\"\"Extract data from URL and return a dataframe\"\"\"\n",
    "    # Check if 'data' folder exists, if not, create it\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    # Download data from Kaggle and save it to 'data' folder\n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.dataset_download_files(dataset_id, path=data_dir, unzip=True)\n",
    "\n",
    "    df = pd.read_excel(f'{data_dir}/Adidas US Sales Datasets.xlsx', sheet_name=\"Data Sales Adidas\", skiprows=range(4), usecols=\"B:N\")\n",
    "    return df\n",
    "\n",
    "def save_to_duckdb(df, table_name, db_path):\n",
    "    \"\"\"Save dataframe to duckdb\"\"\"\n",
    "    conn = duckdb.connect(db_path)\n",
    "    conn.register('df', df)\n",
    "    \n",
    "    # Check if table already exists, if not, create it\n",
    "    tables = conn.execute(\"SHOW TABLES\").fetchall()\n",
    "    if table_name not in [table[0] for table in tables]:\n",
    "        conn.execute(f\"CREATE TABLE {table_name} AS SELECT * FROM df\")\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "# Get range of data dates\n",
    "def data_cleaning_and_saving(df):\n",
    "    df_copy = df.copy()\n",
    "    # Get the start date (oldest date)\n",
    "    start_date = df_copy['Invoice Date'].min()\n",
    "\n",
    "    # Get the last date\n",
    "    last_date = df_copy['Invoice Date'].max()\n",
    "\n",
    "    print(\"Start Date:\", start_date)\n",
    "    print(\"Last Date:\", last_date)\n",
    "\n",
    "    df_copy['Invoice Date'] = pd.to_datetime(df['Invoice Date'])\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Extract data from URL\n",
    "    kaggle_id = 'heemalichaudhari/adidas-sales-dataset'\n",
    "    data_dir = os.path.join('.', 'data')\n",
    "    df = extract_data(kaggle_id,data_dir)\n",
    "    clean_df = data_cleaning_and_saving(df)\n",
    "    \n",
    "    table_name = 'data_sales_adidas'\n",
    "    # Save the cleaned data to DuckDB\n",
    "    save_to_duckdb(df, table_name, f'{data_dir}/adidas.duckdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2b43c17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T03:18:44.249192Z",
     "iopub.status.busy": "2023-10-16T03:18:44.249192Z",
     "iopub.status.idle": "2023-10-16T03:18:45.499810Z",
     "shell.execute_reply": "2023-10-16T03:18:45.499810Z"
    },
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": 1.255167,
     "end_time": "2023-10-16T03:18:45.501928",
     "exception": false,
     "start_time": "2023-10-16T03:18:44.246761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Seasons: ['Winter' 'Spring' 'Summer' 'Fall']\n",
      "Unique Categories: ['Clothing' 'Footwear' 'Outerwear' 'Accessories']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kaggle\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "\n",
    "# Check if 'data' folder exists, if not, create it\n",
    "# Set the path relative to the script\n",
    "def extract_data(dataset_id, data_dir):\n",
    "    \"\"\"Extract data from URL and return a dataframe\"\"\"\n",
    "    # Check if 'data' folder exists, if not, create it\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    # Download data from Kaggle and save it to 'data' folder\n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.dataset_download_files(dataset_id, path=data_dir, unzip=True)\n",
    "\n",
    "    df = pd.read_csv(f'{data_dir}/shopping_trends.csv')\n",
    "    return df\n",
    "\n",
    "def save_to_duckdb(df, table_name, db_path):\n",
    "    \"\"\"Save dataframe to duckdb\"\"\"\n",
    "    conn = duckdb.connect(db_path)\n",
    "    conn.register('df', df)\n",
    "    \n",
    "    # Check if table already exists, if not, create it\n",
    "    tables = conn.execute(\"SHOW TABLES\").fetchall()\n",
    "    if table_name not in [table[0] for table in tables]:\n",
    "        conn.execute(f\"CREATE TABLE {table_name} AS SELECT * FROM df\")\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "def extract_unique_season_and_category(df):\n",
    "    #create a copy\n",
    "    df_copy_2 = df.copy()\n",
    "\n",
    "    # Extract unique values for 'Season' and 'Category' columns\n",
    "    unique_seasons = df_copy_2['Season'].unique()\n",
    "    unique_categories = df_copy_2['Category'].unique()\n",
    "    \n",
    "    print(\"Unique Seasons:\", unique_seasons)\n",
    "    print(\"Unique Categories:\", unique_categories)\n",
    "\n",
    "    return df_copy_2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Extract data from URL\n",
    "    kaggle_id = 'iamsouravbanerjee/customer-shopping-trends-dataset'\n",
    "    data_dir = os.path.join('.', 'data')\n",
    "    df = extract_data(kaggle_id,data_dir)\n",
    "    clean_df = extract_unique_season_and_category(df)\n",
    "    \n",
    "    table_name = 'data_shopping_trends'\n",
    "    # Save the cleaned data to DuckDB\n",
    "    save_to_duckdb(df, table_name, f'{data_dir}/trends.duckdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1501af2",
   "metadata": {
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 0.00109,
     "end_time": "2023-10-16T03:18:45.504929",
     "exception": false,
     "start_time": "2023-10-16T03:18:45.503839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "duration": 6.428539,
   "end_time": "2023-10-16T03:18:45.751704",
   "exception": null,
   "input_path": "C:\\Users\\padro\\AppData\\Local\\Temp\\tmpyrhthcqv.ipynb",
   "output_path": "C:\\Users\\padro\\OneDrive\\Desktop\\ploomber\\sql-etl-analytics\\sql-etl-analytics\\src\\logs\\extract-pipeline.ipynb",
   "parameters": {
    "product": {
     "data": "C:\\Users\\padro\\OneDrive\\Desktop\\ploomber\\sql-etl-analytics\\sql-etl-analytics\\src\\data\\trends.duckdb",
     "nb": "C:\\Users\\padro\\OneDrive\\Desktop\\ploomber\\sql-etl-analytics\\sql-etl-analytics\\src\\logs\\extract-pipeline.ipynb"
    }
   },
   "start_time": "2023-10-16T03:18:39.323165"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}