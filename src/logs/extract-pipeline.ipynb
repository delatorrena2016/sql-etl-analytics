{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd042f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T22:31:52.787987Z",
     "iopub.status.busy": "2023-10-16T22:31:52.787595Z",
     "iopub.status.idle": "2023-10-16T22:31:52.798247Z",
     "shell.execute_reply": "2023-10-16T22:31:52.797603Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 0.019357,
     "end_time": "2023-10-16T22:31:52.800542",
     "exception": false,
     "start_time": "2023-10-16T22:31:52.781185",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# declare a list tasks whose products you want to use as inputs\n",
    "upstream = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae88069",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T22:31:52.806847Z",
     "iopub.status.busy": "2023-10-16T22:31:52.806580Z",
     "iopub.status.idle": "2023-10-16T22:31:52.809838Z",
     "shell.execute_reply": "2023-10-16T22:31:52.809100Z"
    },
    "papermill": {
     "duration": 0.00828,
     "end_time": "2023-10-16T22:31:52.811863",
     "exception": false,
     "start_time": "2023-10-16T22:31:52.803583",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "product = {\"nb\": \"/Users/macpro/Documents/GitHub/sql-etl-analytics/src/logs/extract-pipeline.ipynb\", \"data\": \"/Users/macpro/Documents/GitHub/sql-etl-analytics/src/data/trends.duckdb\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9b81188",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T22:31:52.816604Z",
     "iopub.status.busy": "2023-10-16T22:31:52.816297Z",
     "iopub.status.idle": "2023-10-16T22:32:00.706442Z",
     "shell.execute_reply": "2023-10-16T22:32:00.705480Z"
    },
    "papermill": {
     "duration": 7.898892,
     "end_time": "2023-10-16T22:32:00.712518",
     "exception": false,
     "start_time": "2023-10-16T22:31:52.813626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Date: 2020-01-01 00:00:00\n",
      "Last Date: 2021-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kaggle\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "\n",
    "# Check if 'data' folder exists, if not, create it\n",
    "# Set the path relative to the script\n",
    "def extract_data(dataset_id, data_dir, file_name):\n",
    "    \"\"\"Extract data from URL and return a dataframe\"\"\"\n",
    "    # Check if 'data' folder exists, if not, create it\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    # Download data from Kaggle and save it to 'data' folder\n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.dataset_download_files(dataset_id, path=data_dir, unzip=True)\n",
    "\n",
    "    if \"csv\" in file_name:\n",
    "        df = pd.read_csv(f'{data_dir}/{file_name}')\n",
    "    elif \"xlsx\" in file_name and \"Adidas\" in file_name:\n",
    "        df = pd.read_excel(f'{data_dir}/{file_name}', sheet_name=\"Data Sales Adidas\", skiprows=range(4), usecols=\"B:N\")\n",
    "    else: \n",
    "        df = pd.DataFrame()\n",
    "    return df\n",
    "\n",
    "def extract_unique_season_and_category(df):\n",
    "    #create a copy\n",
    "    df_copy_2 = df.copy()\n",
    "\n",
    "    # Extract unique values for 'Season' and 'Category' columns\n",
    "    unique_seasons = df_copy_2['Season'].unique()\n",
    "    unique_categories = df_copy_2['Category'].unique()\n",
    "    \n",
    "    print(\"Unique Seasons:\", unique_seasons)\n",
    "    print(\"Unique Categories:\", unique_categories)\n",
    "\n",
    "    return df_copy_2\n",
    "\n",
    "def save_to_duckdb(df, table_name, db_path):\n",
    "    \"\"\"Save dataframe to duckdb\"\"\"\n",
    "    conn = duckdb.connect(db_path)\n",
    "    conn.register('df', df)\n",
    "    \n",
    "    # Check if table already exists, if not, create it\n",
    "    tables = conn.execute(\"SHOW TABLES\").fetchall()\n",
    "    if table_name not in [table[0] for table in tables]:\n",
    "        conn.execute(f\"CREATE TABLE {table_name} AS SELECT * FROM df\")\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "# Get range of data dates\n",
    "def data_cleaning_and_saving(df):\n",
    "    df_copy = df.copy()\n",
    "    # Get the start date (oldest date)\n",
    "    start_date = df_copy['Invoice Date'].min()\n",
    "\n",
    "    # Get the last date\n",
    "    last_date = df_copy['Invoice Date'].max()\n",
    "\n",
    "    print(\"Start Date:\", start_date)\n",
    "    print(\"Last Date:\", last_date)\n",
    "\n",
    "    df_copy['Invoice Date'] = pd.to_datetime(df['Invoice Date'])\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Extract data from URL\n",
    "    kaggle_id = 'heemalichaudhari/adidas-sales-dataset'\n",
    "    data_dir = os.path.join('.', 'src/data')\n",
    "    file_name_adidas = \"Adidas US Sales Datasets.xlsx\"\n",
    "    df_adidas = extract_data(kaggle_id,data_dir, file_name_adidas)\n",
    "    clean_df_adidas = data_cleaning_and_saving(df_adidas)\n",
    "    \n",
    "    table_name = 'data_sales_adidas'\n",
    "    # Save the cleaned data to DuckDB\n",
    "    save_to_duckdb(df_adidas, table_name, f'{data_dir}/adidas.duckdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0a19eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T22:32:00.718667Z",
     "iopub.status.busy": "2023-10-16T22:32:00.718395Z",
     "iopub.status.idle": "2023-10-16T22:32:03.401458Z",
     "shell.execute_reply": "2023-10-16T22:32:03.400840Z"
    },
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": 2.68936,
     "end_time": "2023-10-16T22:32:03.405228",
     "exception": false,
     "start_time": "2023-10-16T22:32:00.715868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Seasons: ['Winter' 'Spring' 'Summer' 'Fall']\n",
      "Unique Categories: ['Clothing' 'Footwear' 'Outerwear' 'Accessories']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Extract data from URL\n",
    "    kaggle_id = 'iamsouravbanerjee/customer-shopping-trends-dataset'\n",
    "    file_name_trends  = \"shopping_trends.csv\"\n",
    "    df_trends = extract_data(kaggle_id,data_dir, file_name_trends)\n",
    "    clean_df_trends = extract_unique_season_and_category(df_trends)\n",
    "    \n",
    "    table_name = 'data_shopping_trends'\n",
    "    # Save the cleaned data to DuckDB\n",
    "    save_to_duckdb(clean_df_trends, table_name, f'{data_dir}/trends.duckdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a3159",
   "metadata": {
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 0.001446,
     "end_time": "2023-10-16T22:32:03.409080",
     "exception": false,
     "start_time": "2023-10-16T22:32:03.407634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "duration": 14.030909,
   "end_time": "2023-10-16T22:32:06.048740",
   "exception": null,
   "input_path": "/var/folders/2t/nqb9hcfs07n91h4v5p34slp00000gn/T/tmpj0auy_ih.ipynb",
   "output_path": "/Users/macpro/Documents/GitHub/sql-etl-analytics/src/logs/extract-pipeline.ipynb",
   "parameters": {
    "product": {
     "data": "/Users/macpro/Documents/GitHub/sql-etl-analytics/src/data/trends.duckdb",
     "nb": "/Users/macpro/Documents/GitHub/sql-etl-analytics/src/logs/extract-pipeline.ipynb"
    }
   },
   "start_time": "2023-10-16T22:31:52.017831"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}